{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5aa120-7e6a-407a-8cac-796357cda518",
   "metadata": {},
   "source": [
    "# Setup to do PPO reinforcement learning on Rubik's cubes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50df21ad-b68d-4412-a54e-2f7e7818d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check and use GPU if available if not use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364a38eb-3e76-4ac6-a853-62f2450566bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(num_embeddings=24, embedding_dim=8)\n",
    "lin1 = nn.Linear(8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3305b741-a59e-448f-814c-1ead42d70602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2193, -0.2302,  0.6116,  0.4416, -0.3865, -0.4575, -0.0122,  0.3149,\n",
       "         -0.7319, -0.9678],\n",
       "        [ 0.5248,  0.3983, -0.5535,  0.7822,  0.4990,  0.3748, -1.5316, -0.9435,\n",
       "          0.3232,  0.7618],\n",
       "        [-0.7907,  0.6942, -0.3096, -0.6100, -0.0194,  0.6296, -0.1409, -0.7508,\n",
       "         -0.2710,  0.3607],\n",
       "        [ 0.0145,  0.5076,  0.2045, -0.2945,  0.7087,  0.0409, -0.5489, -0.5676,\n",
       "         -0.2327,  0.0820],\n",
       "        [-0.7582,  0.7239, -0.3279,  0.3541,  0.5619,  0.4855, -1.0863, -1.1435,\n",
       "         -0.1130,  0.0550]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.tensor(np.array([0, 1, 2, 3, 4], np.int64))\n",
    "lin1(emb(v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d036b103-d687-4fde-ad87-0c8dd77b60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# I'm going to parody a transformer architecture a little bit\n",
    "# I would use the torch.nn.TransformerEncoderLayer, but idk if it's doing the positional encoding or not\n",
    "class RubiksEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_size: int = 12,\n",
    "                 output_size: int = 24,\n",
    "                 num_heads: int = 3,\n",
    "                 num_blocks: int = 2,\n",
    "                 hidden_sizes: Tuple[int, ...] = (64, 32)):\n",
    "        super().__init__()\n",
    "        self.__params = None\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=24, embedding_dim=self.embedding_size, max_norm=1.0)\n",
    "        self.positional_encodings = nn.Embedding(num_embeddings=20, embedding_dim=self.embedding_size, max_norm=1.0)\n",
    "        torch.nn.init.xavier_normal_(self.embeddings.weight)\n",
    "        torch.nn.init.xavier_normal_(self.positional_encodings.weight)\n",
    "\n",
    "        self.blocks = [EncoderBlock(self.embedding_size, self.num_heads) for _ in range(self.num_blocks)]\n",
    "\n",
    "        # The hidden layers will pool the output from the encoder blocks into a smaller representation of the cube\n",
    "        self.hidden_layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layer = nn.Linear(self.embedding_size * 20, hidden_sizes[i])\n",
    "            else:\n",
    "                layer = nn.Linear(hidden_sizes[i-1], hidden_sizes[i])\n",
    "            torch.nn.init.xavier_normal_(layer.weight)\n",
    "            self.hidden_layers.append(layer)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], self.output_size)\n",
    "        torch.nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, cubelets):\n",
    "        \"\"\"RubiksEncoder can take any tensor that represents a single array of 20 cubelets (1-D tensor with size=(20,))\n",
    "        or an array containing multiple arrays of cubelets (2-D tensor with size=(X, 20)). A 1-D tensor is treated like\n",
    "        a 2-D tensor of size=(1, 20).\n",
    "        \"\"\"\n",
    "        assert 1 <= len(cubelets.size()) <= 2\n",
    "        assert cubelets.size()[-1] == 20\n",
    "        \n",
    "        embeddings = self.embeddings(cubelets)\n",
    "        positions = torch.tensor(np.arange(0, 20), dtype=torch.int64)\n",
    "        encodings = self.positional_encodings(positions)\n",
    "        encoded = embeddings + encodings\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            embeddings = block.forward(encoded)\n",
    "\n",
    "        # flatten in order to pool\n",
    "        pooled = embeddings.view(-1, self.embedding_size * 20)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            pooled = layer(pooled)\n",
    "        \n",
    "        return self.output_layer(pooled)\n",
    "\n",
    "    def parameters(self):\n",
    "        if self.__params is None:\n",
    "            self.__params = [*self.embeddings.parameters(), *self.positional_encodings.parameters()]\n",
    "            for block in self.blocks:\n",
    "                self.__params.extend(block.parameters())\n",
    "            for layer in self.hidden_layers:\n",
    "                self.__params.extend(layer.parameters())\n",
    "            self.__params.extend(self.output_layer.parameters())\n",
    "        return self.__params\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_size: int,\n",
    "                 num_heads: int):\n",
    "        # do these before anything else\n",
    "        assert embedding_size % num_heads == 0\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding size, number of attention heads, and d_k\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embedding_size // num_heads\n",
    "\n",
    "        # Query, key, value, and output weight matrices for multi-head attention\n",
    "        self.W_q = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        self.W_k = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        self.W_v = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        self.W_o = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        torch.nn.init.xavier_normal_(self.W_q.weight)\n",
    "        torch.nn.init.xavier_normal_(self.W_k.weight)\n",
    "        torch.nn.init.xavier_normal_(self.W_v.weight)\n",
    "        torch.nn.init.xavier_normal_(self.W_o.weight)\n",
    "\n",
    "        # Hidden layers and ReLU activation\n",
    "        self.hidden_layer1 = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        self.hidden_activation = nn.ReLU()\n",
    "        self.hidden_layer2 = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "        torch.nn.init.xavier_normal_(self.hidden_layer1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.hidden_layer2.weight)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # We are reshaping the tensor and then... switching the seq_length and num_heads dimensions?\n",
    "        # seq_length should always be 20\n",
    "        # so we go from (B, 20, embedding_size) to (B, num_heads, 20, d_k)\n",
    "        # And I'm guessing that the matrix multiplication always happens on the last two dimensions\n",
    "        return x.view(-1, 20, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        return x.transpose(1, 2).contiguous().view(-1, 20, self.embedding_size)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # multi-head attention\n",
    "        queries = self.split_heads(self.W_q(embeddings))\n",
    "        keys = self.split_heads(self.W_k(embeddings))\n",
    "        values = self.split_heads(self.W_v(embeddings))\n",
    "        \n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        multi_head = torch.matmul(attn_probs, values)\n",
    "        multi_head_output = self.W_o(self.combine_heads(multi_head))\n",
    "\n",
    "        # residual connection 1\n",
    "        embeddings = nn.functional.normalize(embeddings + multi_head_output)\n",
    "\n",
    "        # feed-forward network\n",
    "        ffnn = self.hidden_layer2(self.hidden_activation(self.hidden_layer1(embeddings)))\n",
    "        # residual connection 2\n",
    "        return nn.functional.normalize(embeddings + ffnn)\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, input_size=24, output_size=18, hidden_layers=(64, 32)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size =  output_size\n",
    "        self.layer_sizes = (self.input_size, *hidden_layers, self.output_size)\n",
    "        self.layers = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            layer = nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1])\n",
    "            self.layers.append(layer)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.__params = []\n",
    "        for layer in self.layers:\n",
    "            self.__params.extend(layer.parameters())\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        output = input_tensor\n",
    "        for i in range(len(self.layers)):\n",
    "            output = self.layers[i](output)\n",
    "            if i < len(self.layers) - 1:\n",
    "                output = self.activation(output)\n",
    "            else:\n",
    "                output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.__params\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, input_size=24, hidden_layers=(64, 32)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = (self.input_size, *hidden_layers, 1)\n",
    "        self.layers = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            layer = nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1])\n",
    "            self.layers.append(layer)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.__params = []\n",
    "        for layer in self.layers:\n",
    "            self.__params.extend(layer.parameters())\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        output = input_tensor\n",
    "        for i in range(len(self.layers)):\n",
    "            output = self.layers[i](output)\n",
    "            if i < len(self.layers) - 1:\n",
    "                output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.__params\n",
    "\n",
    "\n",
    "class RubiksSolver(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = RubiksEncoder()\n",
    "        self.policy_head = PolicyHead()\n",
    "        self.value_head = ValueHead()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.policy(input_tensor)\n",
    "\n",
    "    def policy(self, input_tensor):\n",
    "        return self.policy_head(self.encoder(input_tensor))\n",
    "\n",
    "    def policy_parameters(self):\n",
    "        return [*self.encoder.parameters(), *self.policy_head.parameters()]\n",
    "        \n",
    "    def value(self, input_tensor):\n",
    "        return self.value_head(self.encoder(input_tensor))\n",
    "\n",
    "    def value_parameters(self):\n",
    "        return [*self.encoder.parameters(), *self.value_head.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e1a18741-ed4b-4349-97fb-3ab8f32bdad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RubiksEncoder()\n",
    "solver = RubiksSolver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "24a3c950-44d0-4b64-a3e4-ccb2dffcd289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3b2ec93c-137f-445d-a646-3614a47f627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(solver.policy_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "da938ea6-b86d-40dd-863a-4e87ca989dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(solver.value_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d089e516-d5cf-499d-8efc-29866e0a622d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20696"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in encoder.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ce382849-b3a0-4c2c-8761-aa8ee86dd116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24970"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in solver.policy_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "74c1170c-1cf6-4eab-b085-9fa5688975d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24409"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in solver.value_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a3b76028-f8d0-417e-a482-19f0850eac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gonna make this easier with the crate but for now we hack it manually\n",
    "ACTIONS = (\n",
    "    \"L\", \"L2\", \"L'\",\n",
    "    \"R\", \"R2\", \"R'\",\n",
    "    \"F\", \"F2\", \"F'\",\n",
    "    \"B\", \"B2\", \"B'\",\n",
    "    \"D\", \"D2\", \"D'\",\n",
    "    \"U\", \"U2\", \"U'\",\n",
    ")\n",
    "ROTATIONS = (\n",
    "    'Neutral',\n",
    "    'X', 'X2', 'X3', 'Y', 'Y2', 'Y3', 'Z', 'Z2', 'Z3',\n",
    "    'XY', 'XY2', 'XY3', 'XZ', 'XZ2', 'XZ3',\n",
    "    'X2Y', 'X2Y3', 'X2Z', 'X2Z3',\n",
    "    'X3Y', 'X3Y3', 'X3Z', 'X3Z3',\n",
    ")\n",
    "#INDICES = (\n",
    "#    (2, 2), (2, 3), (2, 4),\n",
    "#    (2, 1), (2, 5),\n",
    "#    (2, 0), (2, -1), (2, -2),\n",
    "#    (1, 2), (1, 4),\n",
    "#    (1, 0), (1, 6),\n",
    "#    (0, 2), (0, 3), (0, 4),\n",
    "#    (0, 1), (0, 5),\n",
    "#    (0, 0), (0, -1), (0, -2),\n",
    "#)\n",
    "INDICES = (\n",
    "    (2, 2), (1, 2), (0, 2),\n",
    "    (2, 1), (0, 1),\n",
    "    (2, 0), (1, 0), (0, 0),\n",
    "    (2, 3), (0, 3),\n",
    "    (2, -1), (0, -1),\n",
    "    (2, 4), (1, 4), (0, 4),\n",
    "    (2, 5), (0, 5),\n",
    "    (2, 6), (1, 6), (0, 6)\n",
    ")\n",
    "\n",
    "def inverse_action(action: int):\n",
    "    return action + 2 - 2 * (action % 3)\n",
    "\n",
    "def cube_to_cubelets(cube):\n",
    "    \"\"\"cube: list of lists of rotation text\"\"\"\n",
    "    cubelets = [ROTATIONS.index(cube[row][col]) for row, col in INDICES]\n",
    "    assert not any(c == -1 for c in cubelets)\n",
    "    return cubelets\n",
    "\n",
    "def parse_cube_and_moves(s: str):\n",
    "    li = s.split('││')\n",
    "    cube = [x.split() for x in li[::2]]\n",
    "    cubelets = cube_to_cubelets(cube)\n",
    "    \n",
    "    moves = ' '.join([x for x in li[1::2] if x.strip()]).replace('(', '').replace(')', '').split()\n",
    "    moves = [ACTIONS.index(m) for m in moves]\n",
    "    \n",
    "    return cubelets, moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "772975bf-4dc0-4a80-90cc-1968e487a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\n",
    "    \"X       X       X       Neutral Neutral Neutral Neutral Neutral                                                             ││(L)                                   ││X       O       X       G       Neutral R       Neutral B                                                                   ││                                      ││X       X       X       Neutral Neutral Neutral Neutral Neutral                                                             ││  \",\n",
    "    \"X2      X2      X2      Neutral Neutral Neutral Neutral Neutral                                                             ││(L2)                                  ││X2      O       X2      G       Neutral R       Neutral B                                                                   ││                                      ││X2      X2      X2      Neutral Neutral Neutral Neutral Neutral                                                             ││     \",\n",
    "    \"X3      X3      X3      Neutral Neutral Neutral Neutral Neutral                                                             ││(L')                                  ││X3      O       X3      G       Neutral R       Neutral B                                                                   ││                                      ││X3      X3      X3      Neutral Neutral Neutral Neutral Neutral                                                             ││    \",\n",
    "    \"Neutral Neutral Neutral Neutral X       X       X       Neutral                                                             ││(R')                                  ││Neutral O       Neutral G       X       R       X       B                                                                   ││                                      ││Neutral Neutral Neutral Neutral X       X       X       Neutral                                                             ││     \",\n",
    "    \"Neutral Neutral Neutral Neutral X2      X2      X2      Neutral                                                             ││(R2)                                  ││Neutral O       Neutral G       X2      R       X2      B                                                                   ││                                      ││Neutral Neutral Neutral Neutral X2      X2      X2      Neutral                                                             ││       \",\n",
    "    \"Neutral Neutral Neutral Neutral X3      X3      X3      Neutral                                                             ││(R)                                   ││Neutral O       Neutral G       X3      R       X3      B                                                                   ││                                      ││Neutral Neutral Neutral Neutral X3      X3      X3      Neutral                                                             ││    \",\n",
    "    \"Neutral Neutral Neutral Neutral Neutral Neutral Neutral Neutral                                                             ││(D)                                   ││Neutral O       Neutral G       Neutral R       Neutral B                                                                   ││                                      ││Z       Z       Z       Z       Z       Z       Z       Z                                                                   ││    \",\n",
    "    \"Neutral Neutral Neutral Neutral Neutral Neutral Neutral Neutral                                                             ││(D2)                                  ││Neutral O       Neutral G       Neutral R       Neutral B                                                                   ││                                      ││Z2      Z2      Z2      Z2      Z2      Z2      Z2      Z2                                                                  ││  \",\n",
    "    \"Neutral Neutral Neutral Neutral Neutral Neutral Neutral Neutral                                                             ││(D')                                  ││Neutral O       Neutral G       Neutral R       Neutral B                                                                   ││                                      ││Z3      Z3      Z3      Z3      Z3      Z3      Z3      Z3                                                                  ││     \",\n",
    "    \"Z       Z       Z       Z       Z       Z       Z       Z                                                                   ││(U')                                  ││Neutral O       Neutral G       Neutral R       Neutral B                                                                   ││                                      ││Neutral Neutral Neutral Neutral Neutral Neutral Neutral Neutral                                                             ││    \",\n",
    "    \"Z2      Z2      Z2      Z2      Z2      Z2      Z2      Z2                                                                  ││(U2)                                  ││Neutral O       Neutral G       Neutral R       Neutral B                                                                   ││                                      ││Neutral Neutral Neutral Neutral Neutral Neutral Neutral Neutral                                                             ││   \",\n",
    "    \"Z3      Z3      Z3      Z3      Z3      Z3      Z3      Z3                                                                  ││(U)                                   ││Neutral O       Neutral G       Neutral R       Neutral B                                                                   ││                                      ││Neutral Neutral Neutral Neutral Neutral Neutral Neutral Neutral                                                             ││    \",\n",
    "    \"Neutral Neutral Y       Y       Y       Neutral Neutral Neutral                                                             ││(F)                                   ││Neutral O       Y       G       Y       R       Neutral B                                                                   ││                                      ││Neutral Neutral Y       Y       Y       Neutral Neutral Neutral                                                             ││     \",\n",
    "    \"Neutral Neutral Y2      Y2      Y2      Neutral Neutral Neutral                                                             ││(F2)                                  ││Neutral O       Y2      G       Y2      R       Neutral B                                                                   ││                                      ││Neutral Neutral Y2      Y2      Y2      Neutral Neutral Neutral                                                             ││    \",\n",
    "    \"Neutral Neutral Y3      Y3      Y3      Neutral Neutral Neutral                                                             ││(F')                                  ││Neutral O       Y3      G       Y3      R       Neutral B                                                                   ││                                      ││Neutral Neutral Y3      Y3      Y3      Neutral Neutral Neutral                                                             ││    \",\n",
    "    \"Y3      Neutral Neutral Neutral Neutral Neutral Y3      Y3                                                                  ││(B)                                   ││Y3      O       Neutral G       Neutral R       Y3      B                                                                   ││                                      ││Y3      Neutral Neutral Neutral Neutral Neutral Y3      Y3                                                                  ││    \",\n",
    "    \"Y2      Neutral Neutral Neutral Neutral Neutral Y2      Y2                                                                  ││(B2)                                  ││Y2      O       Neutral G       Neutral R       Y2      B                                                                   ││                                      ││Y2      Neutral Neutral Neutral Neutral Neutral Y2      Y2                                                                  ││    \",\n",
    "    \"Y       Neutral Neutral Neutral Neutral Neutral Y       Y                                                                   ││(B')                                  ││Y       O       Neutral G       Neutral R       Y       B                                                                   ││                                      ││Y       Neutral Neutral Neutral Neutral Neutral Y       Y                                                                   ││    \",\n",
    "    \"X3Z3    X3Z3    X3Z3    Y       Y       Neutral Neutral Neutral                                                             ││(F L')                                ││X3      O       X3      G       Y       R       Neutral B                                                                   ││                                      ││X3      X3      X3      Y       Y       Neutral Neutral Neutral                                                             ││     \",\n",
    "    \"Neutral Neutral Y       Y       Y       Neutral Neutral Neutral                                                             ││(F D)                                 ││Neutral O       Y       G       Y       R       Neutral B                                                                   ││                                      ││Z       Z       Z       Z       X3Y     X3Y     X3Y     Z                                                                   ││      \",\n",
    "    \"Neutral Neutral Y       Y       Y       Neutral Neutral Neutral                                                             ││(F D2)                                ││Neutral O       Y       G       Y       R       Neutral B                                                                   ││                                      ││X2Y     Z2      Z2      Z2      Z2      Z2      X2Y     X2Y                                                                 ││ \",\n",
    "    \"Neutral Neutral Y       Y       Y       Neutral Neutral Neutral                                                             ││(F D')                                ││Neutral O       Y       G       Y       R       Neutral B                                                                   ││                                      ││XY      XY      XY      Z3      Z3      Z3      Z3      Z3                                                                  ││    \",\n",
    "    \"Neutral Neutral Y       Y       X       X       X       Neutral                                                             ││(F R')                                ││Neutral O       Y       G       X       R       X       B                                                                   ││                                      ││Neutral Neutral Y       Y       XZ      XZ      XZ      Neutral                                                             ││   \",\n",
    "    \"XZ2     XZ2     XZ2     Neutral Neutral Neutral Y2      Y2                                                                  ││(B2 L)                                ││X       O       X       G       Neutral R       Y2      B                                                                   ││                                      ││X       X       X       Neutral Neutral Neutral Y2      Y2                                                                  ││  \",\n",
    "    \"X3      X3      X3      Neutral Neutral Neutral Y2      Y2                                                                  ││(B2 L')                               ││X3      O       X3      G       Neutral R       Y2      B                                                                   ││                                      ││XY2     XY2     XY2     Neutral Neutral Neutral Y2      Y2                                                                  ││    \",\n",
    "    \"Z3      Z3      Z3      Z3      X2Z     X2Z     X2Z     Z3                                                                  ││(B2 U)                                ││Y2      O       Neutral G       Neutral R       Y2      B                                                                   ││                                      ││Y2      Neutral Neutral Neutral Neutral Neutral Y2      Y2                                                                  ││      \",\n",
    "    \"X2      X2      X2Y     Neutral Neutral Neutral Y3      Y3                                                                  ││(B L2)                                ││X2      O       X2Y     G       Neutral R       Y3      B                                                                   ││                                      ││X2      X2      X2Y     Neutral Neutral Neutral Y3      Y3                                                                  ││     \",\n",
    "]\n",
    "\n",
    "data = [parse_cube_and_moves(s) for s in strings]\n",
    "data = [(x, y, inverse_action(y[-1])) for x, y in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7c42b2e3-c163-4185-b324-db6bca13211c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d155ed1e-db3c-4fe1-9253-a25bbbcdbb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0509, 0.0512, 0.0574, 0.0565, 0.0532, 0.0488, 0.0559, 0.0562, 0.0506,\n",
       "         0.0657, 0.0542, 0.0629, 0.0604, 0.0532, 0.0624, 0.0515, 0.0595, 0.0496]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver(torch.tensor([0]*20, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "cb0ebc65-2629-4541-b984-56135fbba813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Janky version of an environment that only takes one step. This is because we have very limited\n",
    "# abilities while I try to make the Rust code usable from Python.\n",
    "class OneShotEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.cubes = [x[0] for x in data]\n",
    "        self.paths = [x[1] for x in data]\n",
    "        self.labels = [x[2] for x in data]\n",
    "        self.selected_data = None\n",
    "        self.action_space = {'n': 18}\n",
    "        self.obs_space = {'n': 20}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        self.selected_data = random.randrange(0, len(self.cubes))\n",
    "        return np.array(self.cubes[self.selected_data], np.int32), None\n",
    "\n",
    "    def step(self, action_ind: int):\n",
    "        \"\"\"Step, then return state, reward, terminated, truncated, and info\"\"\"\n",
    "        #print(action_ind, self.labels[self.selected_data])\n",
    "        if action_ind == self.labels[self.selected_data]:\n",
    "            reward = 100\n",
    "        else:\n",
    "            reward = -100 / 17\n",
    "        #      state (which doesn't matter because the episode ends), reward, terminated=True, truncated=False, info=None\n",
    "        return self.cubes[self.selected_data], reward, True, False, None\n",
    "\n",
    "# I did have a great idea for an environment that gradually increases the difficulty for the agent. Once the agent successfully\n",
    "# solves so many shuffles at a certain depth in a row, it starts shuffling one level deeper. If the agent manages to solve part of\n",
    "# the shuffle, it makes sure the agent can successfully solve cubes at the depth where it failed. For instance, agent gets a shuffle\n",
    "# at depth 5. It makes two correct moves and one incorrect move: depth=5 => success, depth=4 => success, depth=3 => failure. So the\n",
    "# environment starts giving the agent shuffles of depth 3 again to strengthen its memory. In addition, if the agent fails too many\n",
    "# shuffles of the same depth in a row, the environment starts giving shuffles at one depth lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "576dc5b4-d0d6-469a-b3a2-e01ddcd54574",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OneShotEnvironment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "94fdbf12-a7cc-414d-85fd-5e9b925356e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PPO training loop\n",
    "def generate_single_episode(env, agent, mode='train'):\n",
    "    \"\"\"\n",
    "    Generates an episode by executing the current policy in the given env\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    :param env: environment\n",
    "    :param agent: actor-critic network\n",
    "    :param mode: 'train' => use probabilistic, 'evaluate' => use greedy\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    max_t = 50 # max horizon within one episode\n",
    "    state, _ = env.reset()\n",
    "        \n",
    "    for t in range(max_t):\n",
    "        state = torch.from_numpy(state).unsqueeze(0)\n",
    "        probs = agent.policy(Variable(state)) # get each action choice probability with the current policy network\n",
    "        if mode == 'train':\n",
    "            action = np.random.choice(env.action_space['n'], p=np.squeeze(probs.detach().numpy())) # probabilistic\n",
    "        elif mode == 'evaluate':\n",
    "            action = np.argmax(probs.detach().numpy()) # greedy\n",
    "        \n",
    "        # compute the log_prob to use this in parameter update\n",
    "        log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        \n",
    "        # append values\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # take a selected action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "            \n",
    "    return states, actions, rewards, log_probs\n",
    "\n",
    "def train_PPO(env, agent, policy_optimizer, value_optimizer, num_epochs=10, clip_val=0.2, gamma=0.99):\n",
    "    \"\"\"Trains the policy network using PPO\"\"\"\n",
    "    # Generate an episode with the current policy network\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, agent)\n",
    "    T = len(states)\n",
    "    \n",
    "    # Create tensors\n",
    "    states = np.vstack(states).astype(float)\n",
    "    states = torch.LongTensor(states)\n",
    "    actions = torch.LongTensor(actions).view(-1,1)\n",
    "    rewards = torch.FloatTensor(rewards).view(-1,1)\n",
    "    log_probs = torch.FloatTensor(log_probs).view(-1,1)\n",
    "\n",
    "    # Compute total discounted return at each time step\n",
    "    Gs = []\n",
    "    G = 0\n",
    "    for t in range(T-1, -1, -1): # iterate in backward order to make the computation easier\n",
    "        G = rewards[t] + gamma * G\n",
    "        Gs.insert(0, G)\n",
    "    Gs = torch.tensor(Gs).view(-1,1)\n",
    "    \n",
    "    # Compute the advantage\n",
    "    with torch.no_grad():\n",
    "        state_vals = agent.value(states)\n",
    "        A_k = Gs - state_vals\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        # Calculate probability of each action under the updated policy\n",
    "        probs = agent.policy(states)\n",
    "                \n",
    "        # compute the log_prob to use it in parameter update\n",
    "        curr_log_probs = torch.log(torch.gather(probs, 1, actions)) # Use torch.gather(A, 1, B) to select columns from A based on indices in B\n",
    "        \n",
    "        # Calculate ratios r(theta)\n",
    "        ratios = torch.exp(curr_log_probs - log_probs)\n",
    "        \n",
    "        # Calculate two surrogate loss terms in clipped loss\n",
    "        full_loss = ratios * A_k\n",
    "        clipped_loss = torch.clamp(ratios, 1 - clip_val, 1 + clip_val) * A_k\n",
    "        \n",
    "        # Calculate clipped loss value\n",
    "        actor_loss = (-torch.min(full_loss, clipped_loss)).mean() # Need negative sign to run Gradient Ascent\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # Update value net\n",
    "        V = agent.value(states)\n",
    "        critic_loss = nn.MSELoss()(V, Gs)\n",
    "        value_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "def evaluate_PPO(env, agent):\n",
    "    \"\"\"Evaluates the agent\"\"\"\n",
    "    # Generate an episode with the current policy network\n",
    "    _, _, rewards, _ = generate_single_episode(env, agent, mode='evaluate')\n",
    "    total_reward = np.sum(rewards)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2e6a7e89-9b8f-411b-91d1-9f622048bbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [31:24<00:00,  5.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "policy_lr = 5e-4\n",
    "value_lr = 1e-4\n",
    "policy_optimizer = optim.Adam(solver.policy_parameters(), lr=policy_lr)\n",
    "value_optimizer = optim.Adam(solver.value_parameters(), lr=value_lr)\n",
    "n_episodes = 10000\n",
    "\n",
    "for i in trange(n_episodes):\n",
    "    #if i % 100 == 0:\n",
    "    #    print(f'Episode {i}')\n",
    "    train_PPO(env, solver, policy_optimizer, value_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "4919cb8f-cb05-4c3f-ab7d-3eb828c56cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [evaluate_PPO(env, solver) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "38e3cd46-3404-4daf-ab0d-6e77ab579b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.244)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1 if r == 100 else 0 for r in rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4627c7c5-de73-4d95-979b-5c094ea6c033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(19.952941176470596)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "63969fa3-f02f-4827-b607-d9e0ef837b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're definitely going to have to mess with the rewrad functions and then\n",
    "# I'd like to explore the idea of having different training environments that\n",
    "# incrementally train the agent to be better and better.\n",
    "# I also think that we may need a better beginner environment.\n",
    "# I don't think that there is enough data to fit the agent on the 1-turn shuffles alone.\n",
    "# We might have to train it on 3- or 4-turn shuffles at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11b7f8-7586-42a5-8ced-4ed1bdb9bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then of course, obvious things that need to be done are batching the data from the episodes,\n",
    "# playing with the number of epochs per episode, and then after we figure out how to get it to solve\n",
    "# the beginner level stuff, mayyyyyybe we talk about experience replay so it doesn't forget things"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubiks_poetry",
   "language": "python",
   "name": "rubiks_poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
